{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FlowNet.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVdTJsytfFL9"
      },
      "source": [
        "# FlowNet Expert – Deep Learning for Optical Flow Workshop\n",
        "\n",
        "Welcome to FlowNet! In this project, we're going to build a FlowNet algorithm with PyTorch! The idea is simple, given two images, output the optical flow!\n",
        "<p>\n",
        "\n",
        "![flownet](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ_p_REZwjQ1YqfV51j8vQ1qJodRUDRI8Dd7tPuwbWW-tWUQBhKibGi3Bq1ox6SNp5k2ts&usqp=CAU)\n",
        "\n",
        "In this project, we're going to:\n",
        "\n",
        "1.   **Load and Prepare the Dataset** for the Model\n",
        "2.   Define a **FlowNet Architecture**\n",
        "3.   **Train the Model** on KITTI\n",
        "4.   **Run the Model**\n",
        "\n",
        "Just a note before we begin, this code has been adapted from Clement Pinard who authored FlowNet PyTorch. I have been in contact with clement numerous times and he helped me make this course and this code easy to get. \n",
        "<p>\n",
        "\n",
        "For your information, [here is the original repo link](https://github.com/ClementPinard/FlowNetPytorch)\n",
        "\n",
        "[Link to the paper](https://arxiv.org/pdf/1504.06852.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UvxRbhEhgDl"
      },
      "source": [
        "Let's begin with some synchronization and imports!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIyNEHieihui"
      },
      "source": [
        "!wget https://thinkautonomous-flownet.s3.eu-west-3.amazonaws.com/flownet-data.zip && unzip flownet-data.zip && rm flownet-data.zip\n",
        "!mkdir output\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZFfHIHefoc6"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_ynPeGbfXoO"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "from __future__ import division\n",
        "import os.path\n",
        "import os\n",
        "from imageio import imread\n",
        "import numbers\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import random\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import torch.utils.data as data\n",
        "import torch.utils.data as data\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.nn.init import kaiming_normal_, constant_\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9Wav9zINSlK"
      },
      "source": [
        "# Part I - Load and Prepare the Dataset for the Model\n",
        "\n",
        "There are a few Optical Flow Datasets we can use:\n",
        "\n",
        "*   Flying Chairs\n",
        "*   Scene Flow (KITTI)\n",
        "*   Middleburry\n",
        "*   MPI Sintel\n",
        "*   Kinetics\n",
        "\n",
        "For the purpose of this course, we'll use the [KITTI Dataset](http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow) as it's the closest to autonomous driving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS0JmLEIzV6J"
      },
      "source": [
        "images_dataset = sorted(glob.glob(\"dataset/images_2/*.png\"))\n",
        "labels_dataset = sorted(glob.glob(\"dataset/flow_occ/*.png\"))\n",
        "\n",
        "print(len(images_dataset))\n",
        "print(len(labels_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVTt3HHJ8VZS"
      },
      "source": [
        "### 1.1 – Understand input/labels\n",
        "\n",
        "Here's what we want:\n",
        "*   **Input:** A pair of 2 consecutive images\n",
        "*   **Labels:** The Flow Map\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd6VvYSO2Sm9"
      },
      "source": [
        "\"\"\"\n",
        "Make a List of (Img1, Img2, Flow Map)\n",
        "\"\"\"\n",
        "\n",
        "images = []\n",
        "for flow_map in labels_dataset:\n",
        "    root_filename = flow_map[-13:-7]\n",
        "    img1 = os.path.join(\"dataset/images_2/\", root_filename+'_10.png')\n",
        "    img2 = os.path.join(\"dataset/images_2/\", root_filename+'_11.png')\n",
        "    images.append([[img1, img2], flow_map])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X24OY0qa3bU"
      },
      "source": [
        "# images = [[[RGB_image_t, RGB_image_t+1], flowMap], [[RGB_image2_t, RGB_image2_t+1], flowMap2], ...]\n",
        "# 400 RGB_images, 200 flowMaps\n",
        "print(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsYolQyRLCBV"
      },
      "source": [
        "def bgr2rgb(image):\n",
        "    \"\"\"\n",
        "    Convert BGR TO RGB\n",
        "    \"\"\"\n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbviE9fCbFlA"
      },
      "source": [
        "cv2_imshow(bgr2rgb(cv2.imread(images[0][1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBom2OfZuAAZ"
      },
      "source": [
        "# SIMPLE WORKAROUND\n",
        "\n",
        "# cv2.IMREAD_UNCHANGED (flag=-1): reads the image as is from the source. If the source image is an RGB, \n",
        "# it loads the image into array with Red, Green and Blue channels. If the source image is ARGB, it loads \n",
        "# the image with three color components along with the alpha or transparency channel.\n",
        "\n",
        "#TODO: Read the Images with CV2 IMREAD and -1 as a Flag\n",
        "yuv = cv2.imread(images[0][1], -1)\n",
        "# print(\"yuv: \", yuv)\n",
        "# yuv_img = yuv[:,:,2:0:-1].astype(np.float32)   # (375, 1242, 2)\n",
        "# print(\"yuv_img: \", yuv_img)\n",
        "\n",
        "\n",
        "#TODO: Convert from YUV to RGB \n",
        "rgb_map = cv2.cvtColor(yuv, cv2.COLOR_YUV2RGB)\n",
        "# print(\"rgb_map: \", rgb_map)\n",
        "\n",
        "# make the black background become white background \n",
        "rgb_map[np.where((rgb_map==[0,0,0]).all(axis=2))] = [255,255,255]\n",
        "\n",
        "plt.imshow(rgb_map)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYcLLCoEmwLO"
      },
      "source": [
        "!pip install pypng\n",
        "from read_kitti import read_png_file, flow_to_image\n",
        "# Huge Thanks: https://github.com/liruoteng/OpticalFlowToolkit/tree/master/lib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODRSJrlkWHR4"
      },
      "source": [
        "\"\"\"\n",
        "Let's read a triplet of images!\n",
        "\"\"\"\n",
        "idx = random.randint(0,200)\n",
        "\n",
        "image_t0 = bgr2rgb(cv2.imread(images[idx][0][0])) #Read the first image at idx in RGB\n",
        "image_t1 = bgr2rgb(cv2.imread(images[idx][0][1])) #Read the second image at idx in RGB\n",
        "flo_path = images[idx][1] #Get the Flow Path\n",
        "\n",
        "flow_label = read_png_file(flo_path)\n",
        "rgb_map = flow_to_image(flow_label)\n",
        "\n",
        "\"\"\"\n",
        "Visualize the Data\n",
        "\"\"\"\n",
        "\n",
        "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,20))\n",
        "ax1.imshow(image_t0)\n",
        "ax1.set_title('Image t0', fontsize=30)\n",
        "ax2.imshow(image_t1)\n",
        "ax2.set_title('Image t1', fontsize=30)\n",
        "ax3.imshow(rgb_map)\n",
        "ax3.set_title(\"Flow (label)\", fontsize=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiVJ7r4RCktJ"
      },
      "source": [
        "### 1.2 – Split the Data into Train/Test\n",
        "We're going to split the dataset into training and testing. A good ratio would be 80% training and 20% testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm4Ael2nKEA5"
      },
      "source": [
        "def train_test_split(images, default_split=0.8):\n",
        "    \"\"\"\n",
        "    Splits the Dataset Paths into Train/Test\n",
        "    \"\"\"\n",
        "    split_values = np.random.uniform(0,1,len(images)) < default_split # Randomly decides if an image is train or test\n",
        "    train_samples = [sample for sample, split in zip(images, split_values) if split]\n",
        "    test_samples = [sample for sample, split in zip(images, split_values) if not split]\n",
        "    return train_samples, test_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu2jUoJiFUf1"
      },
      "source": [
        "#Call Train/Test split function (easy easy)\n",
        "train_samples, test_samples = train_test_split(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haF5_QQKwIlV"
      },
      "source": [
        "print(len(train_samples))\n",
        "print(len(test_samples))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml6j9bW3C158"
      },
      "source": [
        "### 1.3 – Load the Images\n",
        "\n",
        "So far, we have:\n",
        "*   *images* – a list of triplet paths\n",
        "*   *train_samples* and *test_samples* – these paths into two sets\n",
        "\n",
        "Now, we need to **convert these lists of paths into actual tensors** of images for PyTorch. We'll also need to **perform some transform operations** such as **normalization, random cropping,** etc...\n",
        "<p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gaKhCMpwAJA"
      },
      "source": [
        "def load_flow_from_png(png_path):\n",
        "    '''\n",
        "    This is used to read flow label images from the KITTI Dataset\n",
        "    '''\n",
        "    # The Image is a 16 Bit(uint16) Image. We must read it with OpenCV and \n",
        "    # the flag cv2.IMREAD_UNCHANGED (-1)\n",
        "\n",
        "    # The first channel denotes if the pixel is valid or not (1 if true, 0 otherwise),\n",
        "    # the second channel contains the v-component and the third channel the u-component.\n",
        "    flo_file = cv2.imread(png_path, -1)   # (375, 1242, 3)\n",
        "    flo_img = flo_file[:,:,2:0:-1].astype(np.float32)   # (375, 1242, 2)\n",
        "\n",
        "    # See the README File in the KITTI DEVKIT AND THE FLOW READER FUNCTIONS\n",
        "    # To convert the u-/v-flow into floating point values, convert the value \n",
        "    # to float, subtract 2^15(32768) and divide the result by 64.0\n",
        "    invalid = (flo_file[:,:,0] == 0)\n",
        "    flo_img = flo_img - 32768\n",
        "    flo_img = flo_img / 64\n",
        "\n",
        "    # Valid and Small Flow = 1e-10\n",
        "    flo_img[np.abs(flo_img) < 1e-10] = 1e-10\n",
        "\n",
        "    # Invalid Flow = 0\n",
        "    flo_img[invalid, :] = 0\n",
        "    return flo_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGhJaSV_N0Wy"
      },
      "source": [
        "def KITTI_loader(root, path_imgs, path_flo):\n",
        "    \"\"\"\n",
        "    Returns the Loaded Images in RGB, and the Loaded Optical Flow Labels\n",
        "    \"\"\"\n",
        "    #TODO: Implement the function and return [[img1, img2], flow_image]\n",
        "    imgs = [os.path.join(root, path) for path in path_imgs]\n",
        "    flo = os.path.join(root, path_flo)\n",
        "    # img[:,:,::-1] will do: bgr -> rgb or rgb -> bgr\n",
        "    return [bgr2rgb(cv2.imread(img)).astype(np.float32) for img in imgs], load_flow_from_png(flo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5FzlcKifku4"
      },
      "source": [
        "import flow_transforms\n",
        "\n",
        "div_flow = 20 #Factor by which we divide the output (thus >=1). It makes training more stable to deal with low numbers than big ones.\n",
        "\n",
        "#Normalized for the Flying Chair Dataset (https://github.com/ClementPinard/FlowNetPytorch/issues/101#issuecomment-805222823)\n",
        "input_transform = transforms.Compose([flow_transforms.ArrayToTensor(), transforms.Normalize(mean=[0,0,0], std=[255,255,255]), transforms.Normalize(mean=[0.45,0.432,0.411], std=[1,1,1])])\n",
        "\n",
        "target_transform = transforms.Compose([flow_transforms.ArrayToTensor(),transforms.Normalize(mean=[0,0],std=[div_flow,div_flow])])\n",
        "\n",
        "co_transform = flow_transforms.Compose([flow_transforms.RandomCrop((320,448)), flow_transforms.RandomVerticalFlip(),flow_transforms.RandomHorizontalFlip()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZzU_k7JOnh_"
      },
      "source": [
        "class ListDataset(data.Dataset):\n",
        "    def __init__(self, path_list, transform=None, target_transform=None, co_transform=None, loader=KITTI_loader):\n",
        "        self.root = os.getcwd()\n",
        "        self.path_list = path_list\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.co_transform = co_transform\n",
        "        self.loader = loader\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      \n",
        "        \"\"\"\n",
        "        In Python, __getitem__ is used to read values from a class. For example; read the transformed input files.\n",
        "        Instead of calling the function .read(), we use __getitem__ to directly get the value.\n",
        "        Similarly, __setitem__ can be used to fill values in a class.\n",
        "        \"\"\"\n",
        "        inputs, target = self.path_list[index]\n",
        "        inputs, target = self.loader(self.root, inputs, target)\n",
        "        if self.co_transform is not None:\n",
        "            inputs, target = self.co_transform(inputs, target)\n",
        "        if self.transform is not None:\n",
        "            inputs[0] = self.transform(inputs[0])\n",
        "            inputs[1] = self.transform(inputs[1])\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "        return inputs, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.path_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpFjzrw4Q-fb"
      },
      "source": [
        "train_dataset = ListDataset(train_samples, input_transform, target_transform, co_transform, loader=KITTI_loader)\n",
        "\n",
        "test_dataset = ListDataset(test_samples, input_transform, target_transform, flow_transforms.CenterCrop((370,1224)), loader=KITTI_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZWy6zozKiPl"
      },
      "source": [
        "If you don't understand the ListDataset and how the __ getitem() __ works, a good exercise would be to try and re-code this entire class into something you understand better. Otherwise, let's move on with the Optical Flow Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eujz-vKtfp8F"
      },
      "source": [
        "# Part II – Build a FlowNet Architecture\n",
        "\n",
        "FlowNet has two variations:\n",
        "*   **FlownetS** or Simple, which is a simple version using 2D Convolutions to get to the optical flow computation\n",
        "*   **FlownetC** or Correlated, which adds a correlation layer and process images separately\n",
        "\n",
        "In both, there are two main parts:\n",
        "*   An **Encoder** Part, learning features\n",
        "*   A **Refinement** Part, playing the decoder and creating the output Flow Mask.\n",
        "\n",
        "It looks like we've got some work! In this workshop, we'll build the FlowNet S architecture, as the researchers mentioned it worked best on KITTI! You can find the implementation for the FlowNet C architecture in the course for your information.\n",
        "<p>\n",
        "Here's a look at the flownet S model:\n",
        "\n",
        "![flownets](https://miro.medium.com/max/1400/0*XVygX0wF3enVQJLe.)  \n",
        "\n",
        "And the refinement part:<p>\n",
        "\n",
        "![refinement](https://i1.wp.com/syncedreview.com/wp-content/uploads/2017/09/image-14.png?fit=692%2C268&ssl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GzQjnE1LdWU"
      },
      "source": [
        "### 2.1 – Code the necessary operations\n",
        "\n",
        "Operations we'll need:\n",
        "* Convolutions in 2D (with or without batchnorm)\n",
        "* Output Flow Prediction\n",
        "* Transposed Convolutions\n",
        "* Crop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmFL7wBaX7If"
      },
      "source": [
        "# Define a Convolution with LeakyReLU, and with or without batchnorm\n",
        "def conv(batchNorm, in_planes, out_planes, kernel_size=3, stride=1):\n",
        "    if batchNorm:\n",
        "        #TODO: Code a Convolution in 2D with Batchnorm and LeakyReLU of 0.1\n",
        "        return nn.Sequential(\n",
        "        nn.Conv2d(in_planes, out_planes, kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=False),\n",
        "        nn.BatchNorm2d(out_planes),\n",
        "        nn.LeakyReLU(0.1, inplace=True)\n",
        "        )\n",
        "    else:\n",
        "        #TODO: Code a Convolution in 2D with LeakyReLU of 0.1 and without Batchnorm\n",
        "        return nn.Sequential(\n",
        "        nn.Conv2d(in_planes, out_planes, kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=False),\n",
        "        nn.LeakyReLU(0.1, inplace=True)\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MdJOXrKYJh9"
      },
      "source": [
        "#Define the last convolution (optical flow map prediction)\n",
        "def predict_flow(in_planes):\n",
        "    # TODO: Code a Convolution to predict the output\n",
        "    # The depth of the flow map is 2, which contains (u, v)\n",
        "    return nn.Conv2d(in_planes, 2, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "           # The kernel size in the paper is 5, but we need 3 to make the dimension(width and height) right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mazjo3pYPjv"
      },
      "source": [
        "#Define a Deconvolution\n",
        "def deconv(in_planes, out_planes):\n",
        "    #TODO: Code a Transposed Convolution to upsample the results\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_planes, out_planes, 4, stride=2, padding=1, bias=False),\n",
        "        nn.LeakyReLU(0.1, inplace=True)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ1u-2DMfn5j"
      },
      "source": [
        "#Define a Cropping Operation\n",
        "def crop_like(input, target):\n",
        "    if input.size()[2:] == target.size()[2:]:\n",
        "        # input shape = [B, C, H, W]\n",
        "        # if the width and height of input are the same as target, return input\n",
        "        return input\n",
        "    else:\n",
        "        # otherwise, crop the width and height\n",
        "        return input[:, :, :target.size(2), :target.size(3)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVcaYuqNNP81"
      },
      "source": [
        "\"\"\"\n",
        "test crop_like function\n",
        "\"\"\"\n",
        "input = torch.from_numpy(np.array([[[[1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3]],\n",
        "                  [[1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3]],\n",
        "                  [[1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3]],\n",
        "                  [[1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3]],\n",
        "                  [[1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3], [1,2,3]]]]))\n",
        "target = torch.from_numpy(np.array([[[[9,8,7,6,5,4,3,2,1], [9,8,7,6,5,4,3,2,1], [9,8,7,6,5,4,3,2,1]],\n",
        "                   [[9,8,7,6,5,4,3,2,1], [9,8,7,6,5,4,3,2,1], [9,8,7,6,5,4,3,2,1]],\n",
        "                   [[9,8,7,6,5,4,3,2,1], [9,8,7,6,5,4,3,2,1], [9,8,7,6,5,4,3,2,1]]]]))\n",
        "# crop = crop_like(input, target)\n",
        "print(target.size(2))\n",
        "print(target.size(3))\n",
        "print(\"target shape: \", target.size())\n",
        "print(\"original input shape: \", input.size())\n",
        "print(\"After cropping: \", input[:, :, :target.size(2), :target.size(3)].size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XSKEoTOMkUl"
      },
      "source": [
        "### 2.2 – Create the FlowNet S Model\n",
        "\n",
        "What PyTorch needs to create a model:\n",
        "\n",
        "*   An **__init __() function** with a list of all the operations. Weights can be initialized here.\n",
        "*   A **forward()** function that will take an input and compute the flow. A note: In case of training, we want to return all flows, in case of testing, we only want the output flow.\n",
        "*   **Weights and Biases**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VARY7bulbCV"
      },
      "source": [
        "class FlowNetS(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self,batchNorm=True):\n",
        "        super(FlowNetS,self).__init__()\n",
        "\n",
        "        #ENCODER PART\n",
        "        #TODO: Code the Encoder functions\n",
        "        self.batchNorm = batchNorm\n",
        "        self.conv1 =   conv(self.batchNorm, 6,   64,   7, 2)   # padding = 3, stride = 2  (padding = (kernel_size-1) // 2)\n",
        "        self.conv2 =   conv(self.batchNorm, 64, 128,   5, 2)   # padding = 2, stride = 2\n",
        "        self.conv3 =   conv(self.batchNorm, 128, 256,  5, 2)   # padding = 2, stride = 2\n",
        "        self.conv3_1 = conv(self.batchNorm, 256, 256,  3)      # padding = 1, stride = 1\n",
        "        self.conv4 =   conv(self.batchNorm, 256, 512,  3, 2)   # padding = 1, stride = 2\n",
        "        self.conv4_1 = conv(self.batchNorm, 512, 512,  3)      # padding = 1, stride = 1\n",
        "        self.conv5 =   conv(self.batchNorm, 512, 512,  3, 2)   # padding = 1, stride = 2\n",
        "        self.conv5_1 = conv(self.batchNorm, 512, 512,  3)      # padding = 1, stride = 1\n",
        "        self.conv6 =   conv(self.batchNorm, 512, 1024, 3, 2)   # padding = 1, stride = 2\n",
        "        self.conv6_1 = conv(self.batchNorm, 1024, 1024)     # NOTE: this one doesn't exist in the paper, but it does in their implementation.\n",
        "\n",
        "        # NOTE: in real implementation, it does not have 1x1 convlution\n",
        "\n",
        "        #REFINEMENT PART\n",
        "        #TODO: Code the Decoder functions and Flow Predictions\n",
        "        # NOTE: need to use the same name as the pre-trained weights\n",
        "        self.deconv5 = deconv(1024, 512)\n",
        "        self.deconv4 = deconv(1026, 256)   # inputDepth = 512 + 512(conv5_1) + 2(flow6) = 1026   NOTE: flow6 dosen't exist in the paper\n",
        "        self.deconv3 = deconv(770, 128)    # inputDepth = 256 + 512(conv4_1) + 2(flow5) = 1026\n",
        "        self.deconv2 = deconv(386, 64)     # inputDepth = 128 + 256(conv3_1) + 2(flow4) = 386\n",
        "\n",
        "        self.predict_flow6 = predict_flow(1024)   # NOTE: this one doesn't exist in the paper, but it does in their implementation\n",
        "        self.predict_flow5 = predict_flow(1026)   \n",
        "        self.predict_flow4 = predict_flow(770)\n",
        "        self.predict_flow3 = predict_flow(386)\n",
        "        self.predict_flow2 = predict_flow(194)    # 194 = 64(deconv2) + 128(conv2) + 2(flow3)\n",
        "\n",
        "        self.upsampled_flow6_to_5 = nn.ConvTranspose2d(2, 2, 4, stride=2, padding=1, bias=False)\n",
        "        self.upsampled_flow5_to_4 = nn.ConvTranspose2d(2, 2, 4, stride=2, padding=1, bias=False)\n",
        "        self.upsampled_flow4_to_3 = nn.ConvTranspose2d(2, 2, 4, stride=2, padding=1, bias=False)\n",
        "        self.upsampled_flow3_to_2 = nn.ConvTranspose2d(2, 2, 4, stride=2, padding=1, bias=False)\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                # Initialize the Convolutions with \"He Initialization\" to 0.1 (https://arxiv.org/pdf/1502.01852.pdf)\n",
        "                kaiming_normal_(m.weight, 0.1)\n",
        "                if m.bias is not None:\n",
        "                    # Initialize all bias to 0\n",
        "                    constant_(m.bias, 0)\n",
        "            # Initialize the BatchNorm Convolutions with \"He Initialization\" to 1 (https://arxiv.org/pdf/1502.01852.pdf)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                constant_(m.weight, 1)\n",
        "                constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #TODO: ENCODER\n",
        "        # NOTE: the input size is different than the paper\n",
        "        # x = [batch_size, 6, 320, 448]   \n",
        "        conv1 = self.conv1(x)           # batch_size x 64 x 160 x 224\n",
        "        \n",
        "        conv2 = self.conv2(conv1)       # batch_size x 128 x 80 x 112\n",
        "\n",
        "        conv3 = self.conv3(conv2)       # batch_size x 256 x 40 x 56\n",
        "        conv3_1 = self.conv3_1(conv3)   # batch_size x 256 x 40 x 56\n",
        "\n",
        "        conv4 = self.conv4(conv3_1)     # batch_size x 512 x 20 x 28\n",
        "        conv4_1 = self.conv4_1(conv4)   # batch_size x 512 x 20 x 28\n",
        "\n",
        "        conv5 = self.conv5(conv4_1)     # batch_size x 512 x 10 x 14\n",
        "        conv5_1 = self.conv5_1(conv5)   # batch_size x 512 x 10 x 14\n",
        "\n",
        "        conv6 = self.conv6(conv5_1)     # batch_size x 1024 x 5 x 7\n",
        "        conv6_1 = self.conv6_1(conv6)   # batch_size x 1024 x 5 x 7\n",
        "        \n",
        "        #TODO: REFINEMENT\n",
        "        flow6 = self.predict_flow6(conv6_1)                                 # batch_size x 2 x 5 x 7\n",
        "        flow6_upsampling = self.upsampled_flow6_to_5(flow6)                 # batch_size x 2 x 10 x 14\n",
        "        flow6_upsampling = crop_like(flow6_upsampling, conv5_1)             # batch_size x 2 x 10 x 14\n",
        "\n",
        "        deconv5 = self.deconv5(conv6_1)                                     # batch_size x 512 x 10 x 14\n",
        "        deconv5 = crop_like(deconv5, conv5_1)                               # batch_size x 512 x 10 x 14\n",
        "        concat5 = torch.cat((conv5_1, deconv5, flow6_upsampling), 1)        # batch_size x 1026 x 10 x 14 \n",
        "        flow5 = self.predict_flow5(concat5)                                 # batch_size x 2 x 10 x 14      NOTE: the flow map size is different than the paper\n",
        "        flow5_upsampling = self.upsampled_flow5_to_4(flow5)                 # batch_size x 2 x 20 x 28\n",
        "        flow5_upsampling = crop_like(flow5_upsampling, conv4_1)             # batch_size x 2 x 20 x 28\n",
        "\n",
        "        deconv4 = self.deconv4(concat5)                                     # batch_size x 256 x 20 x 28\n",
        "        deconv4 = crop_like(deconv4, conv4_1)                               # batch_size x 256 x 20 x 28\n",
        "        concat4 = torch.cat((conv4_1, deconv4, flow5_upsampling), 1)        # batch_size x 770 x 20 x 28\n",
        "        flow4 = self.predict_flow4(concat4)                                 # batch_size x 2 x 20 x 28\n",
        "        flow4_upsampling = self.upsampled_flow4_to_3(flow4)                 # batch_size x 2 x 40 x 56\n",
        "        flow4_upsampling = crop_like(flow4_upsampling, conv3_1)             # batch_size x 2 x 40 x 56\n",
        "\n",
        "        deconv3 = self.deconv3(concat4)                                     # batch_size x 128 x 40 x 56\n",
        "        deconv3 = crop_like(deconv3, conv3_1)                               # batch_size x 128 x 40 x 56\n",
        "        concat3 = torch.cat((conv3_1, deconv3, flow4_upsampling), 1)        # batch_size x 386 x 40 x 56\n",
        "        flow3 = self.predict_flow3(concat3)                                 # batch_size x 2 x 40 x 56\n",
        "        flow3_upsampling = self.upsampled_flow3_to_2(flow3)                 # batch_size x 2 x 80 x 112\n",
        "        flow3_upsampling = crop_like(flow3_upsampling, conv2)               # batch_size x 2 x 80 x 112\n",
        "\n",
        "        deconv2 = self.deconv2(concat3)                                     # batch_size x 64 x 80 x 112\n",
        "        deconv2 = crop_like(deconv2, conv2)                                 # batch_size x 64 x 80 x 112\n",
        "        concat2 = torch.cat((conv2, deconv2, flow3_upsampling), 1)          # batch_size x 194 x 80 x 112\n",
        "        flow2 = self.predict_flow2(concat2)                                 # batch_size x 2 x 80 x 112\n",
        "\n",
        "        if self.training:\n",
        "            return flow2,flow3,flow4,flow5,flow6\n",
        "        else:\n",
        "            return flow2\n",
        "\n",
        "    def weight_parameters(self):\n",
        "        return [param for name, param in self.named_parameters() if 'weight' in name]\n",
        "\n",
        "    def bias_parameters(self):\n",
        "        return [param for name, param in self.named_parameters() if 'bias' in name]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "746dLmS5NayY"
      },
      "source": [
        "### 2.3 – Create an Empty FlowNet S Model (with or without pretrained weights)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "celCeoeVeFYK"
      },
      "source": [
        "#Define FlowNet S\n",
        "def flownets(data=None, batchNorm=False):\n",
        "    \"\"\"FlowNetS model architecture from the\n",
        "    \"Learning Optical Flow with Convolutional Networks\" paper (https://arxiv.org/abs/1504.06852)\n",
        "    Args:\n",
        "        data : pretrained weights of the network. will create a new one if not set\n",
        "    \"\"\"\n",
        "    model = FlowNetS(batchNorm=batchNorm)\n",
        "    if data is not None:\n",
        "        model.load_state_dict(data['state_dict'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ekw_YP0BOMEW"
      },
      "source": [
        "If you'd like to create an empty model, simply call the flownets() function without parameters.\n",
        "\n",
        "⚠️ However, **the KITTI Dataset only has 200 data points**. It might be very hard to converge.\n",
        "\n",
        "👉 A good solution is to **load the weights already trained on the Flying Chair dataset** by Clement Pinard, and then **finetune the model on KITTI**. It's called Transfer Learning; and can also work in these cases when the dataset is poor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWz3NPoFjX21"
      },
      "source": [
        "model_to_load = \"models/flownets_bn_EPE2.459.pth.tar\"\n",
        "#model_to_load = \"models/model_best.pth.tar\"\n",
        "checkpoint = torch.load(model_to_load) #if CPU use second parameter: map_location=torch.device(\"cpu\")\n",
        "model = flownets(data=checkpoint, batchNorm=True)\n",
        "\n",
        "print(model)#model = flownets()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pn2GBm7hu86"
      },
      "source": [
        "# Part III - Train the Model on KITTI\n",
        "To train a Deep Learning Model, we'll need:\n",
        "\n",
        "*   Data\n",
        "*   A Model\n",
        "*   Parameters\n",
        "*   A Loss Function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiS3HNkBh_g6"
      },
      "source": [
        "### 3.1 – Define Hyperparameters and Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq6GuSleh6GR"
      },
      "source": [
        "# TODO: fill the hyperparameters\n",
        "arch = \"flownetsbn\"\n",
        "solver = \"adam\"\n",
        "epochs = 200\n",
        "epoch_size = 0\n",
        "batch_size = 64\n",
        "learning_rate = 1e-4\n",
        "workers = 4   # how many subprocesses to use for data loading.\n",
        "pretrained = None\n",
        "bias_decay = 0\n",
        "weight_decay = 4e-4\n",
        "momentum = 0.9\n",
        "milestones= [100, 150, 200] # epoch by which we divide learning rate by 2\n",
        "             \n",
        "save_path = '{},{},{}epochs,b{},lr{}'.format(arch, solver, epochs, batch_size, learning_rate)\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "# When an optimizer is instantiated, parameter group is the as well as a variety of hyperparameters such as the learning rate. \n",
        "# Optimizers are also passed other hyperparameters specific to each optimization algorithm. It can be extremely useful to set up \n",
        "# groups of these hyperparameters, which can be applied to different parts of the model. This can be achieved by creating a \n",
        "# parameter group, essentially a list of dictionaries that can be passed to the optimizer.\n",
        "\n",
        "# The param variable must either be an iterator over a torch.tensor or a Python dictionary specifying a default value of optimization options. \n",
        "# Note that the parameters themselves need to be specified as an ordered collection, such as a list, so that parameters are a consistent sequence\n",
        "param_groups = [{'params': model.bias_parameters(), 'weight_decay': bias_decay},\n",
        "                {'params': model.weight_parameters(), 'weight_decay': weight_decay}]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == \"cuda\":\n",
        "    model = torch.nn.DataParallel(model).cuda()\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "if solver == 'adam':\n",
        "    #TODO: Create a Pytorch ADAM Optimizer\n",
        "    optimizer = torch.optim.Adam(param_groups, lr=learning_rate)\n",
        "elif solver == 'sgd':\n",
        "    #TODO: Create a Pytorch SGD\n",
        "    optimizer = torch.optim.SGD(param_groups, lr=learning_rate, momentum=momentum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEgGYm8tUYbe"
      },
      "source": [
        "Writers can be used to plug values or for tensorboard visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhY4EmpsUVVq"
      },
      "source": [
        "train_writer = SummaryWriter(os.path.join(save_path,'train'))\n",
        "test_writer = SummaryWriter(os.path.join(save_path,'test'))\n",
        "output_writers = []\n",
        "\n",
        "for i in range(3):\n",
        "    output_writers.append(SummaryWriter(os.path.join(save_path,'test',str(i))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Fdk24_iItq"
      },
      "source": [
        "# pin_memory: If True, the data loader will copy Tensors into CUDA pinned memory before returning them.\n",
        "# shuffle: set to True to have the data reshuffled at every epoch\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=workers, pin_memory=True, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,num_workers=workers, pin_memory=True, shuffle=False)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CfrTb24FEoC"
      },
      "source": [
        "for imgs, labels in train_loader:\n",
        "    print(\"{} images with t=0 and t=1\".format(len(imgs)))       # 2 images with t=0 and t=1\n",
        "    print(\"image shape = [B={}, C={}, H={}, W={}]\".format(len(imgs[0]), len(imgs[0][0]), len(imgs[0][0][0]), len(imgs[0][0][0][0])))   # image shape = [B=64, C=3, H=320, W=448]\n",
        "    print(\"label shape = [B={}, C={}, H={}, W={}]\".format(len(labels), len(labels[0]), len(labels[0][0]), len(labels[0][0][0])))       # label shape = [B=64, C=2, H=320, W=448]\n",
        "    input = torch.cat(imgs, 1).to(device)\n",
        "    output = model(input)\n",
        "    print(\"{} predictions\".format(len(output)))     # 5 predictions: flow2, flow3, flow4, flow5, flow6\n",
        "    print(\"predicted flow2 shape = [B={}, C={}, H={}, W={}]\".format(len(output[0]), len(output[0][0]), len(output[0][0][0]), len(output[0][0][0][0])))   # predicted flow2 shape = [B=64, C=2, H=80, W=112]\n",
        "    print(\"predicted flow3 shape = [B={}, C={}, H={}, W={}]\".format(len(output[0]), len(output[0][0]), len(output[1][0][0]), len(output[1][0][0][0])))   # predicted flow3 shape = [B=64, C=2, H=40, W=56]\n",
        "    print(\"predicted flow4 shape = [B={}, C={}, H={}, W={}]\".format(len(output[0]), len(output[0][0]), len(output[2][0][0]), len(output[2][0][0][0])))   # predicted flow4 shape = [B=64, C=2, H=20, W=28]\n",
        "    print(\"predicted flow5 shape = [B={}, C={}, H={}, W={}]\".format(len(output[0]), len(output[0][0]), len(output[3][0][0]), len(output[3][0][0][0])))   # predicted flow5 shape = [B=64, C=2, H=10, W=14]\n",
        "    print(\"predicted flow6 shape = [B={}, C={}, H={}, W={}]\".format(len(output[0]), len(output[0][0]), len(output[4][0][0]), len(output[4][0][0][0])))   # predicted flow6 shape = [B=64, C=2, H=5, W=7]\n",
        "    break\n",
        "\n",
        "for imgs, labels in val_loader:\n",
        "    print(\"{} images with t=0 and t=1\".format(len(imgs)))       # 2 images with t=0 and t=1\n",
        "    # we only has 200 images in total, so the batch size of the val_loader will depend on test_samples, something like 44, 46...\n",
        "    print(\"image shape = [B={}, C={}, H={}, W={}]\".format(len(imgs[0]), len(imgs[0][0]), len(imgs[0][0][0]), len(imgs[0][0][0][0])))   # image shape = [B=, C=3, H=370, W=1224]\n",
        "    print(\"label shape = [B={}, C={}, H={}, W={}]\".format(len(labels), len(labels[0]), len(labels[0][0]), len(labels[0][0][0])))       # label shape = [B=, C=2, H=370, W=1224]\n",
        "    input = torch.cat(imgs, 1).to(device)\n",
        "    output = model(input)\n",
        "    print(\"{} predictions\".format(len(output)))     # 5 predictions: flow2, flow3, flow4, flow5, flow6\n",
        "    print(\"predicted flow2 shape = [B={}, C={}, H={}, W={}]\".format(len(output[0]), len(output[0][0]), len(output[0][0][0]), len(output[0][0][0][0])))   # predicted flow2 shape = [B=, C=2, H=93, W=306]\n",
        "    print(\"predicted flow3 shape = [B={}, C={}, H={}, W={}]\".format(len(output[0]), len(output[0][0]), len(output[1][0][0]), len(output[1][0][0][0])))   # predicted flow3 shape = [B=, C=2, H=47, W=153]\n",
        "    print(\"predicted flow4 shape = [B={}, C={}, H={}, W={}]\".format(len(output[0]), len(output[0][0]), len(output[2][0][0]), len(output[2][0][0][0])))   # predicted flow4 shape = [B=, C=2, H=24, W=77]\n",
        "    print(\"predicted flow5 shape = [B={}, C={}, H={}, W={}]\".format(len(output[0]), len(output[0][0]), len(output[3][0][0]), len(output[3][0][0][0])))   # predicted flow5 shape = [B=, C=2, H=12, W=39]\n",
        "    print(\"predicted flow6 shape = [B={}, C={}, H={}, W={}]\".format(len(output[0]), len(output[0][0]), len(output[4][0][0]), len(output[4][0][0][0])))   # predicted flow6 shape = [B=, C=2, H=6, W=20]\n",
        "\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtrOORO5km0P"
      },
      "source": [
        "### 3.2 – Define the Loss Function as the End Point Error (EPE)\n",
        "\n",
        "Flownet (and most optical flow algorithms) use the end point error (EPE) as a metric for the loss function.\n",
        "It is simply the euclidean distance between the real value (ground truth) and the predicted one.<p>\n",
        "EPE = ![](https://latex.codecogs.com/gif.latex?%5Cinline%20%5Cleft%20%5C%7CV_%7Best%7D%20-%20V_%7Bgt%7D%20%5Cright%20%5C%7C)\n",
        "\n",
        "As the model outputs different flow maps, at different scales, we'll need to create different EPE functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LK2yyw7BFeW"
      },
      "source": [
        "import numpy as np\n",
        "target = np.array([[0,1,2,3,4], [9,0,8,0,7]])\n",
        "mask = (target[:,0] == 0) & (target[:,1]==0)\n",
        "print(~mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EtAjWaBtNVt"
      },
      "source": [
        "def EPE(input_flow, target_flow, sparse=False, mean=True):\n",
        "    #TODO: Define the norm between target and prediction\n",
        "    EPE_map = torch.norm(target_flow - input_flow, p=2, dim=1)\n",
        "\n",
        "    batch_size = EPE_map.size(0)\n",
        "    if sparse:\n",
        "        # invalid flow is defined with both flow coordinates to be exactly 0\n",
        "        mask = (target_flow[:,0] == 0) & (target_flow[:,1] == 0)\n",
        "        # print(\"mask: \", mask)\n",
        "        EPE_map = EPE_map[~mask]\n",
        "        # print(\"EPE_map: \", EPE_map)\n",
        "    if mean:\n",
        "        return EPE_map.mean()\n",
        "    else:\n",
        "        return EPE_map.sum()/batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TGjqGFQpEix"
      },
      "source": [
        "def realEPE(output, target, sparse=False):\n",
        "    \"\"\"\n",
        "    Since the prediction is not the same size as the ground truth,\n",
        "    we need to resize it to the same as the ground truth, and then calculate the EPE\n",
        "    \"\"\"\n",
        "    b, _, h, w = target.size()\n",
        "    upsampled_output = F.interpolate(output, (h,w), mode='bilinear', align_corners=False) # used to resize the output (import torch.nn.functional as F)\n",
        "    return EPE(upsampled_output, target, sparse, mean=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBy4PpK2s8pn"
      },
      "source": [
        "def sparse_max_pool(input, size):\n",
        "    '''\n",
        "    Downsample the input by considering 0 values as invalid.\n",
        "    Unfortunately, no generic interpolation mode can resize a sparse map correctly,\n",
        "    the strategy here is to use max pooling for positive values and \"min pooling\"\n",
        "    for negative values, the two results are then summed.\n",
        "    This technique allows sparsity to be minized, contrary to nearest interpolation,\n",
        "    which could potentially lose information for isolated data points.\n",
        "    '''\n",
        "\n",
        "    positive = (input > 0).float()\n",
        "    negative = (input < 0).float()\n",
        "    output = F.adaptive_max_pool2d(input * positive, size) - F.adaptive_max_pool2d(-input * negative, size)\n",
        "    return output\n",
        "\n",
        "\n",
        "def multiscaleEPE(network_output, target_flow, weights=None, sparse=False):\n",
        "    def one_scale(output, target, sparse):\n",
        "\n",
        "        b, _, h, w = output.size()\n",
        "        if sparse:\n",
        "            target_scaled = sparse_max_pool(target, (h, w))\n",
        "        else:\n",
        "            target_scaled = F.interpolate(target, (h, w), mode='area')\n",
        "        return EPE(output, target_scaled, sparse, mean=False)\n",
        "    \n",
        "\n",
        "    if type(network_output) not in [tuple, list]:\n",
        "        # if the network_output is not a tuple or list, make it a list\n",
        "        network_output = [network_output]\n",
        "    if weights is None:\n",
        "        weights = [0.005, 0.01, 0.02, 0.08, 0.32]  # as in original article\n",
        "    assert(len(weights) == len(network_output))\n",
        "\n",
        "    loss = 0\n",
        "    for output, weight in zip(network_output, weights):\n",
        "        loss += weight * one_scale(output, target_flow, sparse)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoKeZlN5Lga6"
      },
      "source": [
        "### 3.3 Create functions to train and validate\n",
        "\n",
        "We'll begin by using something quite common with PyTorch called an **AverageMeter()**. It is simply a class that **stores the values** for our losses, and that can do an average, median, or whatever we want. **It's quite useful in our case where we have to average a loss over several pixels and several frames.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPj4N7TCuSJ1"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{:.3f} ({:.3f})'.format(self.val, self.avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o08yr4e5vfV1"
      },
      "source": [
        "def save_checkpoint(state, is_best, save_path, filename='checkpoint.pth.tar'):\n",
        "    \"\"\"\n",
        "    Save a checkpoint to continue training after a wifi problem 🙃\n",
        "    \"\"\"\n",
        "    torch.save(state, os.path.join(save_path,filename))\n",
        "    if is_best:\n",
        "        shutil.copyfile(os.path.join(save_path,filename), os.path.join(save_path,'model_best.pth.tar'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-OAEj_dO2VO"
      },
      "source": [
        "\"\"\"\n",
        "The Train() function is actually a function to train on ONE EPOCH.\n",
        "\"\"\"\n",
        "\n",
        "def train(train_loader, model, optimizer, epoch, train_writer):\n",
        "    global n_iter, div_flow\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    flow2_EPEs = AverageMeter()\n",
        "\n",
        "    multiscale_weights = [0.005,0.01,0.02,0.08,0.32] # from output_flow to flow6\n",
        "\n",
        "    epoch_size = len(train_loader)\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        # Go through the entire data loader\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        target = target.to(device)\n",
        "        input = torch.cat(input,1).to(device)\n",
        "\n",
        "        # Forward Pass\n",
        "        output = model(input)  #TODO: Run a Forward Pass\n",
        "\n",
        "        # Since Target pooling is not very precise when sparse,\n",
        "        # take the highest resolution prediction and upsample it instead of downsampling target\n",
        "        h, w = target.size()[-2:]\n",
        "        output = [F.interpolate(output[0], (h,w)), *output[1:]]\n",
        "\n",
        "        # Compute Multiscale EPE (for all predict flows)\n",
        "        loss = multiscaleEPE(output, target, weights=multiscale_weights, sparse=True) #TODO: Run a  Multiscale EPE\n",
        "\n",
        "        # Compute the Output EPE\n",
        "        flow2_EPE = div_flow * realEPE(output[0], target, sparse=True) #TODO: Run the Flow Output EPE (div_flow????\n",
        "\n",
        "        # Record loss and EPE\n",
        "        losses.update(loss.item(), target.size(0))\n",
        "        train_writer.add_scalar('train_loss', loss.item(), n_iter)\n",
        "        flow2_EPEs.update(flow2_EPE.item(), target.size(0))\n",
        "\n",
        "        # compute gradient and do optimization step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % 1 == 0:\n",
        "            # Every 2 steps, print the Loss and EPE\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t Time {3}\\t Data {4}\\t Loss {5}\\t EPE {6}'\n",
        "                  .format(epoch, i, epoch_size, batch_time,\n",
        "                          data_time, losses, flow2_EPEs))\n",
        "        n_iter += 1\n",
        "        if i >= epoch_size:\n",
        "            break\n",
        "    #Return the Average Loss and Average EPE on the Training Set\n",
        "    return losses.avg, flow2_EPEs.avg\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M29R5t1ATknN"
      },
      "source": [
        "def validate(val_loader, model, epoch, output_writers):\n",
        "    global div_flow\n",
        "    batch_time = AverageMeter()\n",
        "    flow2_EPEs = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(val_loader):\n",
        "        #Go through the entire validation loader\n",
        "\n",
        "        target = target.to(device)\n",
        "        input = torch.cat(input,1).to(device)\n",
        "\n",
        "        # Forward Pass\n",
        "        output = model(input) #TODO: Run a forward pass\n",
        "\n",
        "        #Compute the EPE\n",
        "        flow2_EPE = div_flow * realEPE(output, target, sparse=True)  #TODO: Run the output EPE\n",
        "\n",
        "        # record EPE\n",
        "        flow2_EPEs.update(flow2_EPE.item(), target.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i < len(output_writers):  # log first output of first batches\n",
        "            if epoch == 0:\n",
        "                mean_values = torch.tensor([0.45,0.432,0.411], dtype=input.dtype).view(3,1,1)\n",
        "                output_writers[i].add_image('GroundTruth', flow2rgb(div_flow * target[0], max_value=10), 0)\n",
        "                output_writers[i].add_image('Inputs', (input[0,:3].cpu() + mean_values).clamp(0,1), 0)\n",
        "                output_writers[i].add_image('Inputs', (input[0,3:].cpu() + mean_values).clamp(0,1), 1)\n",
        "            output_writers[i].add_image('FlowNet Outputs', flow2rgb(div_flow * output[0], max_value=10), epoch)\n",
        "\n",
        "        if i % 5 == 0:\n",
        "            print('Test: [{0}/{1}]\\t Time {2}\\t EPE {3}'\n",
        "                  .format(i, len(val_loader), batch_time, flow2_EPEs))\n",
        "\n",
        "    print(' * EPE {:.3f}'.format(flow2_EPEs.avg))\n",
        "    # Return Average EPE on Validation Set\n",
        "    return flow2_EPEs.avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEng6J_8ewBP"
      },
      "source": [
        "### 3.4 – Train the Model and Visualize the Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9AfTWhbOcMm"
      },
      "source": [
        "def flow2rgb(flow_map, max_value):\n",
        "    \"\"\"\n",
        "    Used to visualize the output after a forward pass\n",
        "    https://github.com/ClementPinard/FlowNetPytorch/issues/86\n",
        "    \"\"\"\n",
        "    flow_map_np = flow_map.detach().cpu().numpy()\n",
        "    _, h, w = flow_map_np.shape\n",
        "    flow_map_np[:,(flow_map_np[0] == 0) & (flow_map_np[1] == 0)] = float('nan')\n",
        "    rgb_map = np.ones((3,h,w)).astype(np.float32)\n",
        "    if max_value is not None:\n",
        "        normalized_flow_map = flow_map_np / max_value\n",
        "    else:\n",
        "        normalized_flow_map = flow_map_np / (np.abs(flow_map_np).max())\n",
        "    rgb_map[0] += normalized_flow_map[0]\n",
        "    rgb_map[1] -= 0.5*(normalized_flow_map[0] + normalized_flow_map[1])\n",
        "    rgb_map[2] += normalized_flow_map[1]\n",
        "    return rgb_map.clip(0,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmLtwOI14Ui0"
      },
      "source": [
        "save_path = '{},{},{}epochs{},b{},lr{}'.format(arch, solver, epochs, ',epochSize'+str(epoch_size) if epoch_size > 0 else '', batch_size, learning_rate)\n",
        "n_iter = 0\n",
        "best_EPE = -1\n",
        "\n",
        "# We'll start from a model pretrained on \"Flying Chairs\" and finetune it to KITTI\n",
        "\n",
        "save_path = os.path.join(\"models\",save_path)\n",
        "\n",
        "print('=> will save everything to {}'.format(save_path))\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "    scheduler.step()\n",
        "\n",
        "    # Train for one epoch\n",
        "    train_loss, train_EPE = train(train_loader, model, optimizer, epoch, train_writer)  #TODO: Call the Train function\n",
        "    train_writer.add_scalar('mean EPE', train_EPE, epoch)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    with torch.no_grad():\n",
        "        endpointerror = validate(val_loader, model, epoch, output_writers)  #TODO: Call the Validate function\n",
        "    test_writer.add_scalar('mean EPE', endpointerror, epoch)\n",
        "\n",
        "    # Store the best EPE\n",
        "    if best_EPE < 0:\n",
        "        best_EPE = endpointerror\n",
        "\n",
        "    is_best = endpointerror < best_EPE\n",
        "    best_EPE = min(endpointerror, best_EPE)\n",
        "    save_checkpoint({\n",
        "        'epoch': epoch + 1,\n",
        "        'arch': arch,\n",
        "        'state_dict': model.module.state_dict(),\n",
        "        'best_EPE': best_EPE,\n",
        "        'div_flow': div_flow\n",
        "    }, is_best, save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz_Z37hHwG1u"
      },
      "source": [
        "# Part IV – Run the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ddG1M4dk_UI"
      },
      "source": [
        "### 4.1 – On 2 Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvj-o8LXZtQ3"
      },
      "source": [
        "input_transform=transforms.Compose([flow_transforms.ArrayToTensor(),\n",
        "        transforms.Normalize(mean=[0,0,0], std=[255,255,255]),\n",
        "        transforms.Normalize(mean=[0.411,0.432,0.45], std=[1,1,1])\n",
        "    ])\n",
        "\n",
        "network_data = torch.load(\"models/model_best.pth.tar\")\n",
        "#network_data = torch.load(\"models/flownetsbn,adam,200epochs,b64,lr0.001/checkpoint.pth.tar\")\n",
        "div_flow = network_data['div_flow']\n",
        "\n",
        "model = flownets(network_data, batchNorm=True).to(device)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt24naTkpXp_"
      },
      "source": [
        "idx = random.randint(0,len(train_samples))\n",
        "img1_file = train_samples[idx][0][0]\n",
        "img2_file = train_samples[idx][0][1]\n",
        "flow_target = flow_to_image(read_png_file(train_samples[idx][1]))\n",
        "\n",
        "with torch.no_grad():\n",
        "    img1 = input_transform(imread(img1_file))\n",
        "    img2 = input_transform(imread(img2_file))\n",
        "    input_var = torch.cat([img1, img2]).unsqueeze(0)\n",
        "    input_var = input_var.to(device)\n",
        "    output = model(input_var)\n",
        "\n",
        "    for suffix, flow_output in zip(['flow', 'inv_flow'], output):\n",
        "        filename = img1_file[:-4]+\"flow\"\n",
        "        rgb_flow = flow2rgb(div_flow * flow_output, max_value=None)\n",
        "        rgb_flow= (rgb_flow * 255).astype(np.uint8).transpose(1,2,0)\n",
        "\n",
        "f, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(30,20))\n",
        "ax0.imshow(cv2.imread(img1_file)[:,:,::-1])\n",
        "ax0.set_title(\"Original Image\", fontsize=30)\n",
        "ax1.imshow(rgb_flow)\n",
        "ax1.set_title('Prediction', fontsize=30)\n",
        "ax2.imshow(flow_target)\n",
        "ax2.set_title('Ground Truth', fontsize=30)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGOAL7KSj69H"
      },
      "source": [
        "![](https://miro.medium.com/max/592/0*tRzHPmhbfDOfH6qw.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5U7ktNXlBKg"
      },
      "source": [
        "### 4.2 – On a Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdwDpGLMeL6n"
      },
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzHWqMalbqNC"
      },
      "source": [
        "video_idx = 2\n",
        "video_images = sorted(glob.glob(\"videos/video\"+str(video_idx)+\"/*.png\"))\n",
        "result_video = []\n",
        "\n",
        "for idx_run, img in enumerate(video_images):\n",
        "    if idx_run==0:\n",
        "        im1 = imread(img)\n",
        "        idx_run+=1\n",
        "    else:\n",
        "        im2 = imread(img)\n",
        "        with torch.no_grad():\n",
        "            img1 = input_transform(im1)\n",
        "            img2 = input_transform(im2)\n",
        "            input_var = torch.cat([img1, img2]).unsqueeze(0)\n",
        "            input_var = input_var.to(device)\n",
        "\n",
        "            output = model(input_var)\n",
        "\n",
        "            for suffix, flow_output in zip(['flow', 'inv_flow'], output):\n",
        "                rgb_flow = flow2rgb(div_flow * flow_output, max_value=None)\n",
        "                rgb_flow = (rgb_flow * 255).astype(np.uint8).transpose(1,2,0)\n",
        "                result_video.append(cv2.cvtColor(rgb_flow, cv2.COLOR_RGB2BGR))\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpMH93IUqHZb"
      },
      "source": [
        "out = cv2.VideoWriter(\"output/out-\"+str(video_idx)+\".mp4\",cv2.VideoWriter_fourcc(*'MP4V'), 15.0, (311 ,94))\n",
        "\n",
        "for i in range(len(result_video)):\n",
        "    out.write(result_video[i])\n",
        "out.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3Q_WAKIeUmH"
      },
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open(\"output/out-\"+str(video_idx)+\".mp4\",'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=800 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-R-VB2or1WA"
      },
      "source": [
        "video_idx = 3\n",
        "video_images = sorted(glob.glob(\"video\"+str(video_idx)+\"/*.png\"))\n",
        "vid = []\n",
        "for idx_run, img in enumerate(video_images):\n",
        "    vid.append(cv2.imread(img).astype(np.uint8))\n",
        "\n",
        "out = cv2.VideoWriter(\"output/out-\"+str(video_idx)+\".mp4\",cv2.VideoWriter_fourcc(*'MP4V'), 15.0, (1242 ,375))\n",
        "\n",
        "for i in range(len(vid)):\n",
        "    out.write(vid[i])\n",
        "out.release()\n",
        "\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open(\"output/out-\"+str(video_idx)+\".mp4\",'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=800 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}